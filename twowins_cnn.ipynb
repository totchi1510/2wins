{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Drive化にデータを置くと処理が遅くなるため、content化に移動"
      ],
      "metadata": {
        "id": "qOHq18Sy1sbh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9zyWR4nf4J46"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/data_splits /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PhzvzMKF6Lzm"
      },
      "outputs": [],
      "source": [
        "import os, shutil, random\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "#データセットフォルダ\n",
        "dst_root = Path(\"/content/data_splits\")\n",
        "train_root = Path(dst_root, \"train\")\n",
        "val_root   = Path(dst_root, \"val\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "破損しているデータがないかを確認し、必要に応じて破損データを削除する"
      ],
      "metadata": {
        "id": "RwjIerrd1gp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMHK0aQkMuey",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "def check_images(root):\n",
        "    total = ok = empty = corrupted = 0\n",
        "    for p in root.rglob(\"*.*\"):\n",
        "        if p.is_file():\n",
        "            total += 1\n",
        "            try:\n",
        "                if p.stat().st_size == 0:  # サイズ0バイト\n",
        "                    empty += 1\n",
        "                else:\n",
        "                    with Image.open(p) as img:\n",
        "                        img.verify()  # 壊れていないか検証\n",
        "                    ok += 1\n",
        "            except (UnidentifiedImageError, OSError):\n",
        "                corrupted += 1\n",
        "    print(f\"=== {root} ===\")\n",
        "    print(f\"Total     : {total}\")\n",
        "    print(f\"OK        : {ok}\")\n",
        "    print(f\"Empty     : {empty}\")\n",
        "    print(f\"Corrupted : {corrupted}\")\n",
        "    return total, ok, empty, corrupted\n",
        "\n",
        "train_stats = check_images(train_root)\n",
        "val_stats   = check_images(val_root)\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(f\"Train: {train_stats}\")\n",
        "print(f\"Val  : {val_stats}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uVw0ppHyOUwr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 必要に応じて削除したい画像のパスを指定\n",
        "bad_file = Path(\"\")\n",
        "\n",
        "if bad_file.exists():\n",
        "    bad_file.unlink()  # 削除\n",
        "    print(\"Deleted:\", bad_file)\n",
        "else:\n",
        "    print(\"Not found:\", bad_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "データ拡張とデータセットの設定\n",
        "\n",
        "データ拡張の説明\n",
        "\n",
        "*   transforms.Lambda(lambda im: im.convert(\"RGB\"))\n",
        "→ 画像を強制的に RGB 化（チャンネル数を揃えるために必須）\n",
        "\n",
        "*   transforms.RandomResizedCrop(IMG_SIZE, scale=(0.85, 1.0), interpolation=InterpolationMode.BICUBIC)\n",
        "→ 画像のランダムな部分を切り出してリサイズ\n",
        "→ ズームイン・位置の違いに強いモデルを作る\n",
        "\n",
        "*   transforms.RandomHorizontalFlip(p=0.5)\n",
        "→ 50% の確率で左右反転\n",
        "→ 左右対称な対象ならデータ量を実質2倍に増やせる\n",
        "\n",
        "*   transforms.RandomApply([transforms.ColorJitter(...)], p=0.5)\n",
        "→ 50% の確率で明るさ・コントラスト・彩度・色相をランダムに変更\n",
        "→ 照明や色味の変化にロバストになる\n",
        "\n",
        "*   transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1)\n",
        "→ 10% の確率でガウシアンぼかし\n",
        "→ 撮影時のピントずれや軽いブレに耐性を持たせる\n",
        "\n",
        "*   transforms.ToTensor()\n",
        "→ 画像を PyTorch のテンソル形式 [C,H,W] に変換\n",
        "\n",
        "*   transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "→ ImageNet の平均・標準偏差で正規化\n",
        "→ 事前学習済みモデル（ResNet, ViTなど）に合わせるため必須\n",
        "\n",
        "*   transforms.RandomErasing(p=0.25, scale=(0.02,0.08), ratio=(0.3,3.3))\n",
        "→ 25% の確率で画像の一部を矩形で塗りつぶす\n",
        "→ 欠損や汚れがあっても識別できるようにする\n"
      ],
      "metadata": {
        "id": "cq0E37_r70Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "IMAGENET_MEAN=(0.485,0.456,0.406); IMAGENET_STD=(0.229,0.224,0.225)\n",
        "IMG_SIZE=224\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.85, 1.0),\n",
        "                                 interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 部品の左右対称が前提。NGなら外す\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.15,0.15,0.15,0.02)], p=0.5),\n",
        "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),  # ぼかし弱\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.02,0.08), ratio=(0.3,3.3)),   # 小さめの欠損\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Lambda(lambda im: im.convert(\"RGB\")),\n",
        "    transforms.Resize(int(IMG_SIZE*1.15), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n"
      ],
      "metadata": {
        "id": "sybNk61UP5Xj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "\n",
        "train_ds = datasets.ImageFolder(str(train_root), transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(str(val_root),   transform=val_tfms)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "1hLjYTXyQAES"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルと損失関数、最適化関数の定義"
      ],
      "metadata": {
        "id": "PEzN8xMf2fi_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4S159wtFGVw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "# GPU を使う\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Resnet\n",
        "model = models.resnet50(pretrained=True)\n",
        "# 出力を2クラス用に置き換え\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# 損失関数 & optimizer設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの学習"
      ],
      "metadata": {
        "id": "1u8WjfwbaQUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "epochs = 50  # 最大エポック数\n",
        "best_val_acc = 0.0\n",
        "val_loss_his = []\n",
        "ckpt_path = \"/content/best_model_Resnet50.pt\" # 重みファイルの保存先\n",
        "\n",
        "patience = 2   # 何エポック改善がなければ止めるか\n",
        "no_improve = 0 # 改善がないエポック数カウンタ\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss_sum += loss.item() * imgs.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    run_loss, run_correct, run_total = 0.0, 0, 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        run_loss += loss.item() * imgs.size(0)\n",
        "        run_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "        run_total += labels.size(0)\n",
        "\n",
        "    train_loss = run_loss / run_total\n",
        "    train_acc  = run_correct / run_total\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "    val_loss_his.append(val_loss)\n",
        "\n",
        "    print(f\"[Epoch {epoch:02d}] \"\n",
        "          f\"train_loss={train_loss:.4f}  train_acc={train_acc*100:.2f}%  \"\n",
        "          f\"val_loss={val_loss:.4f}  val_acc={val_acc*100:.2f}%\")\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    # ベスト更新時だけ保存\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        print(f\" Best updated (val_loss={val_loss:.4f}) -> {ckpt_path}\")\n",
        "\n",
        "    if epoch > 1 and val_loss_his[epoch-1] > val_loss_his[epoch-2]:\n",
        "          no_improve += 1\n",
        "    else:\n",
        "          no_improve = 0\n",
        "\n",
        "    # Early stopping\n",
        "    if no_improve >= patience:\n",
        "        print(\"⏹ Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "print(f\"Done. Best val_acc = {best_val_acc:.4f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MxhOFAQdTVm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果指標を算出"
      ],
      "metadata": {
        "id": "5rESGlfo5RdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "weight_path = \"/content/best_model_Resnet50.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 学習時と同様にモデルを定義\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "state = torch.load(weight_path, map_location=device)\n",
        "model.load_state_dict(state)\n",
        "model.to(device).eval()\n",
        "\n",
        "all_probs, all_labels, per_image_latencies = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "\n",
        "        # GPU計測の前後で同期\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        t0 = time.perf_counter()\n",
        "\n",
        "        logits = model(imgs)                  # [B, C]\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        t1 = time.perf_counter()\n",
        "\n",
        "        batch_time = t1 - t0\n",
        "        per_image_latencies.extend([batch_time / imgs.size(0)] * imgs.size(0))\n",
        "\n",
        "        probs = torch.softmax(logits, dim=1)  # [B, C]\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "probs = np.concatenate(all_probs, axis=0)  # [N, C]\n",
        "y     = np.concatenate(all_labels, axis=0) # [N]\n",
        "\n",
        "# class_to_idx: {'bad': 0, 'good': 1}\n",
        "POSITIVE_LABEL = 0         # bad のラベルID\n",
        "POSITIVE_IDX   = 0         # bad の確率列\n",
        "y_true_bin = (y == POSITIVE_LABEL).astype(int)        # [N] 0/1\n",
        "y_score    = probs[:, POSITIVE_IDX].astype(float)     # [N]\n",
        "\n",
        "# PR / AP\n",
        "precision, recall, thr = precision_recall_curve(y_true_bin, y_score)\n",
        "ap = average_precision_score(y_true_bin, y_score)\n",
        "\n",
        "# Recall=100%を満たす運用閾値 τ* と P@R=100%\n",
        "pos_scores = y_score[y_true_bin == 1]\n",
        "if len(pos_scores) == 0:\n",
        "    raise ValueError(\"不良（陽性）が1つもないため評価できません。\")\n",
        "\n",
        "tau_star = float(pos_scores.min())                 # 閾値：不良の最小スコア\n",
        "y_pred_op = (y_score >= tau_star).astype(int)      # 1=不良で予測\n",
        "\n",
        "TP = int(((y_pred_op == 1) & (y_true_bin == 1)).sum())\n",
        "FP = int(((y_pred_op == 1) & (y_true_bin == 0)).sum())\n",
        "TN = int(((y_pred_op == 0) & (y_true_bin == 0)).sum())\n",
        "FN = int(((y_pred_op == 0) & (y_true_bin == 1)).sum())\n",
        "\n",
        "recall_at_tau    = TP / (TP + FN) if (TP + FN) else 0.0\n",
        "precision_at_tau = TP / (TP + FP) if (TP + FP) else 0.0\n",
        "\n",
        "# レイテンシ統計\n",
        "lat_ms = np.array(per_image_latencies) * 1000.0\n",
        "p50 = np.percentile(lat_ms, 50)\n",
        "p95 = np.percentile(lat_ms, 95)\n",
        "\n",
        "print(\"=== 運用点(Recall=100%を満たす閾値) ===\")\n",
        "print(f\"閾値 τ*                  : {tau_star:.6f}\")\n",
        "print(f\"Recall@τ* (期待=1.0)     : {recall_at_tau:.4f}\")\n",
        "print(f\"Precision@Recall=100%    : {precision_at_tau:.4f}\")\n",
        "print(\"\\n=== 一枚当たりの推論時間 ===\")\n",
        "print(f\"p50={p50:.2f} ms, p95={p95:.2f} ms\")\n",
        "\n",
        "# PRカーブ表示\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, )\n",
        "plt.scatter([1.0], [precision_at_tau], s=80, marker='o',\n",
        "            label=f'R=100% point (τ*={tau_star:.3f})')\n",
        "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision–Recall curve')\n",
        "plt.grid(True); plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "NV6bxCAKLw7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}